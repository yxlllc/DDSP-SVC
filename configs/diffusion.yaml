data:
  f0_extractor: 'parselmouth' # 'parselmouth', 'dio', 'harvest', or 'crepe'
  f0_min: 65 # about C2
  f0_max: 800 # about G5
  sampling_rate: 44100
  block_size: 512 # Equal to hop_length
  duration: 2 # Audio duration during training, must be less than the duration of the shortest audio clip
  encoder: 'hubertsoft' # 'hubertsoft', 'hubertbase', 'hubertbase768', 'contentvec', 'contentvec768' or 'contentvec768l12' or 'cnhubertsoftfish'
  cnhubertsoft_gate: 10 # only use with cnhubertsoftfish
  encoder_sample_rate: 16000
  encoder_hop_size: 320
  encoder_out_channels: 256 # 768 if using 'hubertbase768', 'contentvec768' or 'contentvec768l12' encoder
  encoder_ckpt: pretrain/hubert/hubert-soft-0d54a1f4.pt
  speaker_encoder: 'ge2e' # only use use_speaker_encoder
  speaker_encoder_sample_rate: 16000
  speaker_encoder_out_channels: 256
  speaker_encoder_ckpt: 'pretrain/speaker_encoder/best_model.pth.tar' # only use with use_speaker_encoder
  speaker_encoder_config: 'pretrain/speaker_encoder/config.json' # only use with use_speaker_encoder
  speaker_encoder_mode: 'each_wav' # 'each_wav' or 'each_spk', only use with use_speaker_encoder, 'each_wav' will retain the spk_emb of each wav, 'each_spk' will average the spk_emb from all wav of the same speaker into one spk_emb
  train_path: data/train # Create a folder named "audio" under this path and put the audio clip in it
  valid_path: data/val # Create a folder named "audio" under this path and put the audio clip in it
  extensions: # List of extension included in the data collection
    - wav
model:
  type: 'Diffusion'
  n_layers: 20
  n_chans: 512
  n_hidden: 256
  use_pitch_aug: true  
  n_spk: 1 # max number of different speakers
  use_speaker_encoder: false
device: cuda
vocoder:
  type: 'nsf-hifigan'
  ckpt: 'pretrain/nsf_hifigan/model'
infer:
  speedup: 10
  method: 'dpm-solver' # 'pndm' or 'dpm-solver'
env:
  expdir: exp/diffusion-test
  gpu_id: 0
train:
  num_workers: 2 # If your cpu and gpu are both very strong, set to 0 may be faster!
  amp_dtype: fp32 # fp32, fp16 or bf16 (fp16 or bf16 may be faster if it is supported by your gpu)
  batch_size: 48
  cache_all_data: true # Save Internal-Memory or Graphics-Memory if it is false, but may be slow
  cache_device: 'cpu' # Set to 'cuda' to cache the data into the Graphics-Memory, fastest speed for strong gpu
  cache_fp16: true
  epochs: 100000
  interval_log: 10
  interval_val: 2000
  interval_force_save: 10000
  lr: 0.0002
  decay_step: 100000
  gamma: 0.5
  weight_decay: 0
  save_opt: false
